{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Loan Granting Prediction\n",
    "## Adura ABIONA, PhD (UNSW)\n",
    "### 12 February, 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "This project is based on the Cortana Intelligence Gallery Competition . This competition was part of the requirement for Data Science Professional Project for **Microsoft Professional Program in Data Science(MPP-DS)**. The competition started on 12/12/2016 and ended on 1/29/2017. The evaluation criterium to achieve a passing grade in the accompanying course is that **Private Score Highest** has to be at least **70%**. \n",
    "\n",
    "### Short Description for the Required Task \n",
    "This competition concerns loan data. When a customer applies for a loan, banks and other credit providers use statistical models to determine whether or not to grant the loan based on the likelihood of the loan being repaid. The factors involved in determining this likelihood are complex, and extensive statistical analysis and modelling are required to predict the outcome for each individual case. The task is to implement a model that predicts loan repayment or default based on the data provided.\n",
    "\n",
    "### Dataset Information\n",
    "The dataset used in this competition consists of synthetic data that was generated specifically for use in this project. The data is designed to exhibit similar characteristics to genuine loan data. The dataset consisting of over 111,000 loan records to determine the best way to predict whether a loan applicant will fully repay or default on a loan.\n",
    "\n",
    "More information on it can be found at [**Loan Granting Binary Classification December 2016**](https://gallery.cortanaintelligence.com/Competition/1ad7a6df99794816b9bc071e27d46b10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os, string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use('ggplot')\n",
    "%matplotlib inline \n",
    "print(pd.__version__)\n",
    "\n",
    "import seaborn as sns  # Seaborn, useful for graphics\n",
    "# JB's favorite Seaborn settings for notebooks\n",
    "rc = {'lines.linewidth': 2, \n",
    "      'axes.labelsize': 18, \n",
    "      'axes.titlesize': 18, \n",
    "      'axes.facecolor': 'DFDFE5'}\n",
    "sns.set_context('notebook', rc=rc)\n",
    "sns.set_style('darkgrid', rc=rc)\n",
    "# This Function takes as input a custom palette\n",
    "flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoan = pd.read_csv(\"LoansTrainingDataset.csv\")\n",
    "print(dfLoan.shape)\n",
    "dfLoan.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check for the number of missing values in each column as isnull() \n",
    "dfLoan.apply(lambda x: sum(x.isnull()),axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are **111,107 rows and 19 columns** in this dataset out of which **59,003 rows** more than 50% of the records have missing values in the *Months since last delinquent*\" column alone. This is a lot, therefore it will not contribute much information to the modeling as such I will drop this column.\n",
    "\n",
    "There are still **21,338 rows** that have missing values in the ***Credit Score*** and ***Annual Income*** columns. This is a somewhat significant number which could be treated by repalcing them with mean/median of each  column or by forward/backward filling methods. However, considering this number and the importance of annual income and credit score in the ability of an applicant to pay back the loan, using above methods would affect the modelling result negatively.  Therefore, it is advisable to drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfLoan.shape)\n",
    "# Removes columns with missing values \n",
    "dfLoan1 = dfLoan.drop(labels=['Months since last delinquent'], axis=1)\n",
    "\n",
    "# Removes rows with missing values    \n",
    "dfLoan1 = dfLoan1[~pd.isnull(dfLoan1).any(axis=1)]\n",
    "\n",
    "print(dfLoan1.shape)\n",
    "dfLoan1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Checking feature types\n",
    "def coltype(df):\n",
    "        datatype1 = df.select_dtypes(exclude = ['object']).dtypes\n",
    "        print('**** Number of numeric features = {} ****'.format(datatype1.count()))\n",
    "        print(datatype1)\n",
    "    \n",
    "        datatype2 = df.select_dtypes(include = ['object']).dtypes\n",
    "        print('\\n**** Number of non-numeric features = {} ****'.format(datatype2.count()))\n",
    "        print(datatype2)\n",
    "        \n",
    "coltype(dfLoan1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# List unique elements of each feature and also detects datatype misclassification \n",
    "def uniqueCatList(df):\n",
    "    print('Frame shape: {}'.format(df.shape))\n",
    "    for col in df.select_dtypes(include = ['object']).columns.tolist():\n",
    "        print('\\nNumber of unique members[{}] = {}'.format(col,  len(df[col].unique().tolist())))\n",
    "        print(df[col].unique())\n",
    "#numeric_feats = train.dtypes[train.dtypes != \"object\"].index\n",
    "uniqueCatList(dfLoan1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfLoan1.groupby('Years in current job')['Loan ID'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfLoan1.shape)\n",
    "dfLoan1 = dfLoan1[dfLoan1['Years in current job'] != 'n/a']\n",
    "print(dfLoan1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the above function, we are able to detect that ***Monthly Debt*** and ***Maximum Open Credit*** faetures are numeric but have some elements that are wrongly input as string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List non-numeric elements in numeric features\n",
    "def DetectNonNumeric(dfx, nnCol):\n",
    "    for colx in nnCol:\n",
    "        nonNumList = []\n",
    "        for el in dfx[colx].unique().tolist():\n",
    "            try:      \n",
    "                float(el)\n",
    "            except:\n",
    "                nonNumList.append(el)\n",
    "        print('{}: {}'.format(colx, nonNumList))\n",
    "        \n",
    "DetectNonNumeric(dfLoan1, ['Monthly Debt','Maximum Open Credit'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code block below munges the ***Monthly Debt*** and ***Maximum Open Credit*** columns to the correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# coverting feature element  to numeric and forcing string to nan\n",
    "dfLoan1['Monthly Debt'] = [float(str(row).replace(\"$\",\"\"))  if '$' in str(row) else row   for row in dfLoan1['Monthly Debt']]\n",
    "dfLoan1['Monthly Debt'] = pd.to_numeric(dfLoan1['Monthly Debt'], errors='coerce')\n",
    "\n",
    "dfLoan1['Maximum Open Credit'] = pd.to_numeric(dfLoan1['Maximum Open Credit'], errors='coerce')\n",
    "dfLoan1['Maximum Open Credit'].fillna(dfLoan1['Maximum Open Credit'].mean(), inplace=True) # fill missing values with mean\n",
    "\n",
    "DetectNonNumeric(dfLoan1, ['Monthly Debt','Maximum Open Credit']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing duplicate rows\n",
    "print(dfLoan1.shape)\n",
    "dfLoan1.drop_duplicates(['Loan ID'], inplace = True) \n",
    "print(dfLoan1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoan1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the categorical features\n",
    "datacols = dfLoan1.select_dtypes(exclude = ['object']).columns.tolist()\n",
    "print(datacols)\n",
    "len(datacols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacol1 = ['Current Loan Amount', 'Credit Score', 'Annual Income', 'Monthly Debt', \n",
    "            'Years of Credit History', 'Number of Open Accounts']\n",
    "sns.pairplot(dfLoan1, vars=datacol1, size=3, kind=\"reg\") #size=3, diag_kind=\"kde\", kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datacol2 = ['Current Loan Amount', 'Number of Credit Problems', 'Current Credit Balance', \n",
    "            'Maximum Open Credit', 'Bankruptcies', 'Tax Liens']\n",
    "sns.pairplot(dfLoan1, vars=datacol2, size=3, kind=\"reg\") #size=3, diag_kind=\"kde\", kind=\"reg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 'Current Loan Amount', 'Credit Score', 'Annual Income', 'Monthly Debt', 'Years of Credit History', 'Number of Open Accounts'\n",
    "def plot_outlier(x,y):\n",
    "    fig,axs=plt.subplots(1,2,figsize=(8,3))\n",
    "    sns.boxplot(y,orient='v',ax=axs[0])\n",
    "    sns.regplot(x,y,ax=axs[1])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx0 = dfLoan1[dfLoan1['Current Loan Amount'] <  0]\n",
    "print(dfx0.shape)\n",
    "dfx1 = dfLoan1[dfLoan1['Current Loan Amount'] <  0.5E8]\n",
    "print(dfx1.shape)\n",
    "dfx2 = dfLoan1[dfLoan1['Current Loan Amount'] >  0.5E8]\n",
    "dfx2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfLoan1 = dfLoan1[dfLoan1['Annual Income'] <  0.2E7]\n",
    "plot_outlier(dfLoan1['Current Loan Amount'], dfLoan1['Annual Income'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfLoan1 = dfLoan1[dfLoan1['Monthly Debt'] <  15000]\n",
    "\n",
    "plot_outlier(dfLoan1['Current Loan Amount'], dfLoan1['Monthly Debt'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfLoan1 = dfLoan1[dfLoan1['Current Credit Balance'] <  1000000]\n",
    "\n",
    "plot_outlier(dfLoan1['Current Loan Amount'], dfLoan1['Current Credit Balance'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfLoan1 = dfLoan1[dfLoan1['Annual Income'] <  3.0E7]\n",
    "\n",
    "plot_outlier(dfLoan1['Current Loan Amount'], dfLoan1['Maximum Open Credit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfLoan1.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plotcorrFloat(df):\n",
    "    corr = df[['Current Loan Amount', 'Credit Score', 'Annual Income', 'Monthly Debt', 'Years of Credit History', \n",
    "               'Number of Open Accounts', 'Number of Credit Problems', 'Current Credit Balance', \n",
    "               'Maximum Open Credit', 'Bankruptcies', 'Tax Liens']].corr()\n",
    "    colormap = plt.cm.viridis\n",
    "    plt.figure(figsize=(12, 12))\n",
    "    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n",
    "    return sns.heatmap(corr, vmax=1, square=True, linewidths=0.1, cmap=colormap, linecolor='white', annot=True)\n",
    "\n",
    "# 'Current Loan Amount', 'Credit Score', 'Annual Income', 'Monthly Debt', 'Years of Credit History', 'Number of Open Accounts'\n",
    "plotcorrFloat(dfLoan1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model prediction\n",
    "For the model prediction, I will be using a classifier of gradient boosted decision trees to predict whether a loan applicant will fully repay or default on a loan. In particular, I will use XGBoost. It is an algorithm that has recently been dominating applied machine learning and [**Kaggle competitions**](https://www.kaggle.com/competitions) for structured data.\n",
    "\n",
    "[**XGBoost**](http://xgboost.readthedocs.io/en/latest/model.html) is an implementation of gradient boosted decision trees designed for speed and performance. [**Szilard Pafka's excellent benchmark**](https://github.com/szilard/benchm-ml) of a variety of machine learning libraries attest to XGBoost fast computation speed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import model libraries\n",
    "import xgboost as xgb\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "import scipy.stats as st\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "#Import evaluation metrics\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, roc_auc_score,  roc_curve, auc, precision_score, recall_score\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "\"\"\"\n",
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True')\n",
    "    plt.xlabel('Predicted')\n",
    "  \"\"\"  \n",
    "# Compute ROC curve and ROC area for each class\n",
    "def plot_ROCcurve(Ytest, pred, title = \"ROC Curve\"):\n",
    "    fpr = dict()\n",
    "    tpr = dict()\n",
    "    roc_auc = dict()\n",
    "    fpr, tpr, _ = roc_curve(Ytest, pred)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Ordinal Encoding to Categoricals\n",
    "We need to convert some features into categorical group to make processing simpler. The columns Vessel_ID represents categorical feature. However, because it is an integer, it is initially parsed as continuous number. It is also required to encode features like DayNight with a string category since XGBoost (like all of the other machine learning algorithms in Python) requires every feature vector to include only digits. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# categorizing features that need it\n",
    "CategLx = ['Loan Status', 'Term', 'Years in current job', 'Home Ownership', 'Purpose'] # Categorical features \n",
    "dfLoan2 = dfLoan1  \n",
    "for fea in dfLoan2[CategLx]: # Loop through all columns in the dataframe\n",
    "    dfLoan2[fea] = pd.Categorical(dfLoan2[fea]).codes # Convert to categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Separate target from other features: input (X) features, target (y) feature & label (Z) feature\n",
    "#Z = dfLoan1['Loan ID']\n",
    "Y = dfLoan2['Loan Status']\n",
    "X = dfLoan2.drop(['Loan Status', 'Loan ID', 'Customer ID'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numCol = dfLoan2.select_dtypes(exclude = ['object']).columns.tolist()\n",
    "print(numCol)\n",
    "print(len(numCol))\n",
    "\n",
    "catCol = dfLoan2.select_dtypes(include = ['object']).columns.tolist()\n",
    "print(catCol)\n",
    "len(catCol)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split data into train and test sets\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.45, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Next step is to separate features into numeric/categorical groups to make scaling  which is needed for accurate prediction for numeric features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "CategLs = ['Term', 'Years in current job', 'Home Ownership', 'Purpose'] # Categorical features \n",
    "X_train_Cat = X_train[CategLs]\n",
    "X_test_Cat = X_test[CategLs]\n",
    "\n",
    "X_train_Num = X_train.drop(CategLs, axis=1)\n",
    "X_test_Num = X_test.drop(CategLs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaler = StandardScaler() # create scaler object\n",
    "scaler.fit(X_train_Num) # fit with the training data ONLY\n",
    "X_train_Num = scaler.transform(X_train_Num) \n",
    "X_test_Num = scaler.transform(X_test_Num) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.drop(CategLs, axis=1).head().columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# names = datacols\n",
    "X_train_tot = pd.concat([pd.DataFrame(X_train_Num, columns = datacols), X_train_Cat.reset_index(drop=True)], axis=1)\n",
    "X_test_tot = pd.concat([pd.DataFrame(X_test_Num, columns = datacols), X_test_Cat.reset_index(drop=True)], axis=1)\n",
    "print(X_train_Num.shape)\n",
    "print(X_train_tot.shape)\n",
    "X_train_tot.head()\n",
    "X_train_tot.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "one_to_left = st.beta(10, 1)  \n",
    "from_zero_positive = st.expon(0, 50)\n",
    "\n",
    "params = {  \n",
    "    \"n_estimators\": st.randint(3, 40),\n",
    "    \"max_depth\": st.randint(3, 40),\n",
    "    \"learning_rate\": st.uniform(0.05, 0.4),\n",
    "    \"colsample_bytree\": one_to_left,\n",
    "    \"subsample\": one_to_left,\n",
    "    \"gamma\": st.uniform(0, 10),\n",
    "    'reg_alpha': from_zero_positive,\n",
    "    \"min_child_weight\": from_zero_positive\n",
    "}\n",
    "\n",
    "xgbreg = XGBClassifier(nthread=-1)\n",
    "rsCV = RandomizedSearchCV(xgbreg, params, n_jobs=1)  \n",
    "rsCV.fit(X_train_tot, Y_train)\n",
    "rsCV.best_params_, rsCV.best_score_\n",
    "\n",
    "clf = XGBClassifier(**rsCV.best_params_)\n",
    "clf.fit(X_train_tot, Y_train)\n",
    "\n",
    "predXtest = clf.predict(X_test_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes, normalize=False, title='Confusion matrix', cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix. Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "print (\"\\nModel Report\")\n",
    "print (\"Accuracy : %.4g\" % accuracy_score(Y_test, predXtest))\n",
    "print (\"AUC Score (Train): %f\" % roc_auc_score(Y_test, predXtest))\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(predXtest, Y_test)\n",
    "# Class names\n",
    "class_names = ['Negative', 'Positive']\n",
    "# Plot non-normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, title='Confusion matrix, without normalization')\n",
    "# Plot normalized confusion matrix\n",
    "plt.figure()\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True, title='Normalized confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(confusion_matrix(Y_test, predXtest), 'Loan Confusion Matrix', savefilename='Loan CM.png')\n",
    "plot_ROCcurve(Y_test, predXtest, \"Loan ROC Curve\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Print model report:\n",
    "feat_imp = pd.Series(clf.booster().get_fscore()).sort_values(ascending=False)\n",
    "feat_imp.plot(kind='barh', title='Feature Importances')\n",
    "plt.ylabel('Feature Importance Score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
